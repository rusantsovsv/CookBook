"""
Вы работаете со строками Unicode и хотите убедиться, что все эти строки имеют
одинаковое внутреннее представление.
В Unicode некоторые символы могут быть представлены несколькими допусти-
мыми кодирующими последовательностями. Рассмотрим пример:
"""
s1 = 'Spicy Jalape\u00f1o'
s2 = 'Spicy Jalapen\u0303o'

print(s1, s2, sep='\n')
print(s1 == s2)
print(len(s1))
print(len(s2))

"""
Здесь текст «Spicy Jalapeño» представлен в двух формах. Первая использует пол-
ноценный символ «ñ» (U+00F1). Вторая использует латинскую букву «n», за кото-
рой следует дополняющий символ «~» (U+0303).
Такие множественные представления становятся проблемой для программ, ко-
торые занимаются сравнением строк. Чтобы это исправить, вы должны сначала
нормализовать текст, то есть привести его к стандартному представлению с по-
мощью модуля unicodedata:
"""

import unicodedata

t1 = unicodedata.normalize('NFC', s1)
t2 = unicodedata.normalize('NFC', s2)
print(t1 == t2)
print(ascii(t1))

t3 = unicodedata.normalize('NFD', s1)
t4 = unicodedata.normalize('NFD', s2)

print(t3 == t4)
print(ascii(t3))

"""
Первый аргумент, передаваемый в normalize(), определяет режим нормализа-
ции текста. NFC означает, что символы должны быть полноценными (то есть по
возможности использовать только одну кодирующую последовательность). NFD
означает, что символы должны быть декомпозированными, то есть разделенны-
ми на комбинирующиеся символы.
Python также поддерживает режимы нормализации NFKC и NFKD, которые до-
бавляют возможности совместимости, позволяющие работать с определенными
типами символов. Например:
"""

s = '\ufb01'    # один символ
print(s)

print(unicodedata.normalize('NFD', s))

# обратите внимание на разбивку объединения букв
print(unicodedata.normalize('NFKD', s))
print(unicodedata.normalize('NFKC', s))

"""
Нормализация – это важная часть любой программы, в которой присутствует не-
обходимость обработки текста в Unicode разумным и единообразным способом.
Это особенно важно, когда обрабатываемые строки поступают из пользователь-
ского ввода, кодировку которого вы практически никак не контролируете.
Нормализация также может быть важной частью чистки и фильтрации текста.
Предположим, вы хотите удалить из текста диакритические знаки (возможно, для
цели поиска совпадений):
"""

t1 = unicodedata.normalize('NFD', s1)
print(''.join(c for c in t1 if not unicodedata.combining(c)))